% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CalibrationPlot.R
\name{calibrationplot}
\alias{calibrationplot}
\title{Calibration Plot Function}
\arguments{
\item{learners}{\code{list} of \code{\link[mlr3:Learner]{Learner}}\cr
List of trained learners to be evaluated.}

\item{task}{\code{\link[mlr3:Task]{Task}}\cr
Task containing the data to be used for prediction and evaluation.}

\item{bins}{\code{integer(1)}\cr
Number of bins to use when grouping predicted probabilities. Default is \code{11}.}

\item{smooth}{\code{logical(1)}\cr
Whether to plot a smoothed calibration curve using LOESS. Default is \code{FALSE}.}

\item{CI}{\code{logical(1)}\cr
Whether to include confidence intervals when \code{smooth} is \code{TRUE}. Default is \code{FALSE}.}

\item{rug}{\code{logical(1)}\cr
Whether to add a rug plot to show individual prediction points. Default is \code{FALSE}.}
}
\value{
A \code{ggplot} object displaying the reliability curves.
}
\description{
Generates calibration plots, also known as reliability curves, for one or more classification learners.
This function evaluates how well the predicted probabilities from the learners are calibrated with the true outcomes.
}
\details{
For each learner, the function predicts probabilities on the given task.
The predicted probabilities are divided into bins, and within each bin,
the mean predicted probability and the mean observed outcome are calculated.
These values are then plotted to assess calibration.
}
\examples{
# Example usage of calibrationplot
set.seed(1)
data("Sonar", package = "mlbench")
task = as_task_classif(Sonar, target = "Class", positive = "M")
splits = partition(task)
task_train = task$clone()$filter(splits$train)
task_test = task$clone()$filter(splits$test)
# Initialize the uncalibrated learner
learner_uncal <- lrn("classif.xgboost", nrounds = 50, predict_type = "prob")

# Initialize the calibrated learner
rsmp <- rsmp("cv", folds = 5)
learner_cal <- as_learner(PipeOpCalibration$new(learner = learner_uncal,
                                                method = "beta",
                                                rsmp = rsmp))

# Set ID's for the learners
learner_uncal$id <- "Uncalibrated Learner"
learner_cal$id <- "Calibrated Learner"

# Train the learners
learner_uncal$train(task_train)
learner_cal$train(task_train)

# List the Learners you want to plot
lrns = list(learner_uncal, learner_cal)

# Plot the reliability curve
calibrationplot(lrns, task_test, smooth = TRUE)

}
\references{
Niculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning (pp. 625-632).
}
\concept{Plotting Functions}
